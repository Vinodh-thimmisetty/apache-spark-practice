{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, DateType\n",
    "from pyspark.sql.functions import col, broadcast, shuffle\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.sql.autoBroadcastJoinThreshold', 10485760),('spark.sql.join.preferSortMergeJoin', 'false')])\n",
    "spark = SparkSession.builder.appName(\"Spark SQL Join Practice\").config(conf=conf).getOrCreate()\n",
    "# spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8d659",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emp_df_schema=StructType([\n",
    "    StructField('id', IntegerType(), False , None),\n",
    "    StructField('name', StringType(), False , None),\n",
    "    StructField('join_year', IntegerType(), False , None),\n",
    "    StructField('club_id', IntegerType(), True , None),\n",
    "    StructField('salary', IntegerType(), True , None),\n",
    "    StructField('manager_id', IntegerType(), False, None)    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "emp_df_data=[\n",
    "    (1, 'Mourinho', 2010, 1001,  20000, -1),\n",
    "    (2, 'Guardiola', 2004, 1002,  20000, -1),\n",
    "    (3, 'Klopp', 2015, 1003,  30000, -1),\n",
    "    (4, 'Salah', 2017, 1003,  40000, 3),\n",
    "    (5, 'Messi', 2000, 1002,  50000, 1),\n",
    "    (6, 'Ronaldo', 2009, 1001, 50000, 2),\n",
    "    (7, 'Zlatan', 1996, 1012,  50000, -1),\n",
    "    (8, 'Pele', 2009, 1021, 50000, -1)\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_df_data, emp_df_schema)\n",
    "# emp_df.printSchema()\n",
    "# emp_df.show(truncate=False)\n",
    "emp_df.createOrReplaceTempView('EMPLOYEE_VW')\n",
    "\n",
    "club_df_schema= StructType([\n",
    "    StructField('club_id', IntegerType(), False, None),\n",
    "    StructField('club_name', StringType(), False, None),\n",
    "    StructField('club_country', StringType(), False, None),\n",
    "    StructField('club_continent', StringType(), False, None)\n",
    "])\n",
    "\n",
    "club_df_data = [\n",
    "    (1001, 'RealMadrid', 'Spain', 'Europe'),\n",
    "    (1002, 'Barcelona', 'Spain', 'Europe'),\n",
    "    (1003, 'Liverpool', 'UK', 'Europe'), \n",
    "    (1004, 'Dortmund', 'Germany', 'Europe'),\n",
    "    (1005, 'Manchester City', 'UK', 'Europe'),\n",
    "    (1006, 'Newcastle United', 'UK', 'Europe'),\n",
    "    (1007, 'Bocca Juniors', 'Brazil', 'South America')\n",
    "\n",
    "]\n",
    "\n",
    "club_df = spark.createDataFrame(club_df_data, club_df_schema)\n",
    "# club_df.printSchema()\n",
    "# club_df.show(truncate=False)\n",
    "club_df.createOrReplaceTempView('CLUB_VW')\n",
    "\n",
    "print(' Players/Managers matching with Clubs ')\n",
    "spark.sql(\"Select E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'inner').select('name', 'club_name').show()\n",
    "\n",
    "print(' All available Players/Managers and All available Clubs ')\n",
    "spark.sql(\"Select E.name, C.club_name FROM EMPLOYEE_VW E FULL OUTER JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'outer').select('name', 'club_name').show()\n",
    "\n",
    "print(' All Players/Managers irrespective of Club ')\n",
    "spark.sql(\"Select distinct E.name FROM EMPLOYEE_VW E LEFT OUTER JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'left').select('name').distinct().show()\n",
    "\n",
    "print(' All Clubs irrespective of Players/Managers ')\n",
    "spark.sql(\"Select distinct C.club_name FROM EMPLOYEE_VW E RIGHT OUTER JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'right').select('club_name').distinct().show()\n",
    "\n",
    "print(' Players/Managers matching with Clubs, but dont care about Club Details ')\n",
    "spark.sql(\"Select * FROM EMPLOYEE_VW E LEFT SEMI JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'leftsemi').show()\n",
    "\n",
    "print(' Players/Managers NOT matching with any Clubs, but dont care about Club Details ')\n",
    "spark.sql(\"Select * FROM EMPLOYEE_VW E LEFT ANTI JOIN CLUB_VW C ON E.club_id = C.club_id;\").show()\n",
    "emp_df.join(club_df, emp_df['club_id'] == club_df['club_id'], 'leftanti').show()\n",
    "\n",
    "\n",
    "print(' How many different ways a Players/Manager can match to Club.')\n",
    "spark.sql(\"Select distinct E.name, C.club_name FROM EMPLOYEE_VW E CROSS JOIN CLUB_VW C;\").show(100)\n",
    "emp_df.join(club_df).select('name', 'club_name').distinct().orderBy(col('name')).show(100)\n",
    "\n",
    "print(' Players reporting to Manager ')\n",
    "spark.sql(\"Select distinct E.name as Player, M.name as Manager FROM EMPLOYEE_VW E JOIN EMPLOYEE_VW M ON E.manager_id = M.id;\").show()\n",
    "emp_df.alias('p').join(emp_df.alias('m'), col('p.manager_id') == col('m.id'), 'inner').distinct().select(col('p.name').alias('Player'), col('m.name').alias('Manager')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f4d11",
   "metadata": {},
   "source": [
    "[Spark Joins](https://blog.clairvoyantsoft.com/apache-spark-join-strategies-e4ebc7624b06)\n",
    "\n",
    "[Shuffle Hash Join](https://www.hadoopinrealworld.com/how-does-shuffle-hash-join-work-in-spark/)\n",
    "[Shuffle Sort Merge Join](https://www.hadoopinrealworld.com/how-does-shuffle-sort-merge-join-work-in-spark/)\n",
    "\n",
    "\n",
    "1. Broadcast Hash Join (BKJ)  \n",
    "    If one of the TWO joined tables is small enough to fit in memory, collect() and broadcast it to all EXECUTORS.\n",
    "    To Disable : spark.sql.autoBroadcastJoinThreshold=-1\n",
    "    Max allowable Size: spark.sql.autoBroadcastJoinThreshold=******** (default 10MB)\n",
    "    Note:\n",
    "        Based on which side of join HINT is provided, that will be broadcast (ignores autoBroadcastJoinThreshold)\n",
    "        Consider only when you know one of the table is small and less than autoBroadcastJoinThreshold size\n",
    "    \n",
    "    case#1  \n",
    "           LEFT (LARGE)       VS         RIGHT (SMALL)  \n",
    "        Parition#1                    Parition#1      \n",
    "        {barcelona, spain}            {2, Gaurdiola}\n",
    "        {barcelona, spain}            {3, Klopp}  \n",
    "        {liverpool, uk}               \n",
    "        {liverpool, uk}\n",
    "        Parition#2\n",
    "        {Dortmund, Germany}            \n",
    "        {Bocca Juniors, Brazil}       \n",
    "        {Manchester city, uk}          \n",
    "        {NewCastle, uk}       \n",
    "        \n",
    "        \n",
    "        \n",
    "      After Broadcasting:\n",
    "          LEFT             VS         RIGHT   \n",
    "        Parition#1                    Parition#1      \n",
    "        {barcelona, spain}            {2, Gaurdiola}\n",
    "        {barcelona, spain}    ->      {3, Klopp}  \n",
    "        {liverpool, uk}               \n",
    "        {liverpool, uk}\n",
    "        Parition#2                    Parition#1  \n",
    "        {Dortmund, Germany}            {2, Gaurdiola}\n",
    "        {Bocca Juniors, Brazil}  ->    {3, Klopp}\n",
    "        {Manchester city, uk}          \n",
    "        {NewCastle, uk} \n",
    "              \n",
    "       \n",
    "      After Hash Joining (Smallest table will be hashed and Large table will use it for lookup): \n",
    "       Let's say for INNER Join - Bucket#1 stores {Guardiola} & Bucket#2 stores {Klopp}  \n",
    "        for(CLUB in LEFT SIDE) {\n",
    "           // Get club Hash and search in Bucket#1 or Bucket#2\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "     case#2\n",
    "          LEFT (SMALL)     VS         RIGHT (LARGE)   \n",
    "        Parition#1                   Parition#1\n",
    "        {realmadrid, spain}           {2, Gaurdiola}\n",
    "        {NewCastle, uk}               {3, Klopp}  \n",
    "        {liverpool, uk}               {5, Messi}\n",
    "                                      {4, Salah}\n",
    "                                      Parition#2\n",
    "                                      {1, Mourinho}\n",
    "                                      {6, Ronaldo}\n",
    "                                      {7, Zlatan}\n",
    "                                      {8, Pele}\n",
    "    \n",
    "        \n",
    "          \n",
    "      After Broadcasting:\n",
    "          LEFT             VS         RIGHT   \n",
    "        Parition#1                    Parition#1      \n",
    "        {realmadrid, spain}            {2, Gaurdiola}\n",
    "        {NewCastle, uk}    <--         {3, Klopp}  \n",
    "        {liverpool, uk}                 {5, Messi}\n",
    "                                        {4, Salah}\n",
    "        Parition#1                    Parition#2  \n",
    "        {realmadrid, spain}            {1, Mourinho}\n",
    "        {NewCastle, uk}     <--        {6, Ronaldo}\n",
    "        {liverpool, uk}                {7, Zlatan}\n",
    "                                       {8, Pele}\n",
    "              \n",
    "       \n",
    "      After Hash Joining (Smallest table will be hashed and Large table will use it for lookup): \n",
    "       Let's say for INNER Join - Bucket#1 stores {spain} & Bucket#2 stores {uk}  \n",
    "        for(Player in RIGHT SIDE) {\n",
    "           // Get Player Hash and search in Bucket#1 or Bucket#2\n",
    "        }\n",
    "        \n",
    "        \n",
    "2. Shuffle Hash Join (SHJ)\n",
    "    Large Tables Join; COSTLY; spark.sql.join.preferSortMergeJoin=FALSE\n",
    "    Only For EQUI-Joins\n",
    "    Shuffle(parition based on Join Column and move same CLUB_ID data into same parition)     \n",
    "    Parition#1\n",
    "            LEFT             VS          RIGHT   \n",
    "        {barcelona, spain}            {2, Gaurdiola}\n",
    "        {barcelona, spain}            {5, Messi}  \n",
    "        {liverpool, uk}               {4, Salah}\n",
    "        {liverpool, uk}               {3, Klopp}\n",
    "        {Dortmund, Germany}            \n",
    "        {Bocca Juniors, Brazil}       \n",
    "        {Manchester city, uk}          \n",
    "        {NewCastle, uk}\n",
    "        \n",
    "    Partition#2\n",
    "            LEFT             VS          RIGHT   \n",
    "        {realmadrid, spain}            {1, Mourinho}\n",
    "        {readmadrid, spain}            {6, Ronaldo}  \n",
    "                                       {7, Zlatan}\n",
    "                                       {8, Pele}   \n",
    "\n",
    "    Hash(Generate a single Node Hash value for each Partition. )\n",
    "    Parition#1\n",
    "        RIGHT Side is small, So create a Buckets(Hash) for lookup Map<ID, CLUB>\n",
    "        Let's say for INNER Join - Bucket#1 stores {Messi,Guardiola} & Bucket#2 stores {Salah, Klopp}  \n",
    "        for(Player in LEFT SIDE) {\n",
    "           // Get Player Hash and search in Bucket#1 or Bucket#2\n",
    "        }\n",
    "\n",
    "    Partition#3\n",
    "        LEFT Side is small, So create a Buckets(Hash) for lookup Map<ID, PLAYER>\n",
    "        E.g: Only One Bucket#1 that have {realmadrid}\n",
    "        for(Club in RIGHT SIDE) {\n",
    "           // Get Club's hash and Search in Players Buckets#1\n",
    "        }\n",
    "\n",
    "\n",
    "3. Shuffle Sort Merge Join (SMJ) - Both Large Tables\n",
    "    spark.sql.join.preferSortMergeJoin=true + spark.sql.autoBroadcastJoinThreshold=-1\n",
    "    Shuffle - same as above\n",
    "    Sort - Sort each individual partition data.\n",
    "    Merge - Continue Compare and Merge LEFT and RIGHT sides till LEFT side data not matched to RIGHT..\n",
    "\n",
    "\n",
    "4. Shuffle Replicate Nested Loop\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spark Optimized Join based on Join Type and HINTS')\n",
    "# spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")\n",
    "# spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "# spark.sql(\"Select /*+ BROADCAST(C) */ E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").explain(True)\n",
    "# spark.sql(\"Select /*+ SHUFFLE_HASH(C) */ E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").explain(True)\n",
    "# spark.sql(\"Select /*+ SHUFFLE_MERGE(C) */ E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").explain(True)\n",
    "# spark.sql(\"Select /*+ SHUFFLE_REPLICATE_NL(C) */ E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").explain(True)\n",
    "# spark.sql(\"Select /*+ BROADCAST(C),SHUFFLE_MERGE(C),SHUFFLE_HASH(C), SHUFFLE_REPLICATE_NL(C)  */ E.name, C.club_name FROM EMPLOYEE_VW E JOIN CLUB_VW C ON E.club_id = C.club_id;\").explain(True)\n",
    "\n",
    "emp_df.join(broadcast(club_df), emp_df['club_id'] == club_df['club_id'], 'inner').select('name', 'club_name').explain(True)\n",
    "emp_df.join(broadcast(club_df), emp_df['club_id'] == club_df['club_id'], 'inner').select('name', 'club_name').explain(True)\n",
    "emp_df.join(broadcast(club_df), emp_df['club_id'] == club_df['club_id'], 'inner').select('name', 'club_name').explain(True)\n",
    "emp_df.join(broadcast(club_df), emp_df['club_id'] == club_df['club_id'], 'inner').select('name', 'club_name').explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce4e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
